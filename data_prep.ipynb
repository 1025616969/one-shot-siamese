{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/processed/'\n",
    "train_dir = data_dir + 'background/'\n",
    "test_dir = data_dir + 'evaluation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data\n",
    "\n",
    "The train data consists of 40 alphabets and 12 drawers.\n",
    "\n",
    "In each alphabet folder, there is 1 folder for each character in the alphabet. In each character folder, there are 20 images of this character drawn by each of the 20 drawers.\n",
    "\n",
    "- To create a pair belonging to the same class, we need to select 2 drawers from the same character dir, for any character dir inside the permitted language dirs.\n",
    "- To create a pair belonging to different classes, we need to select 2 drawers from different character dirs, for character dir inside the permitted language dirs.\n",
    "\n",
    "Ok so I'm gonna have a counter that starts from 0 and increments up to 30k. At each iteration, if the iteration number is even, I sample a like pair. If it is odd, I sample a dissimilar pair. \n",
    "\n",
    "- To sample a like pair, first I randomly select an alphabet from the list of training alphabets (uniform proba). Then, I randomly select a character from the list of characters in this alphabet (uniform proba). Then, I randomly select 2 drawers from the list of training drawers (uniform sampling) and select their corresponding image from the subdirectory.\n",
    "- To sample a dissimilar pair, I can do 2 things. The first method is to uniformally sample 1 alphabet, then uniformally sample 2 characters in the alphabet, uniformally sample 1 drawer (or 2) and index 1 drawer per alphabet to obtain 2 different characters from the same alphabet. The second method is to uniformally sample 2 alphabets, uniformally sample a character in each alphabet, uniformally sample a drawer per alphabet (or the same drawer), and obtain 2 different characters from 2 different alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 alphabets.\n"
     ]
    }
   ],
   "source": [
    "# get list of alphabets\n",
    "original_alphabets = [os.path.join(train_dir, x) for x in next(os.walk(train_dir))[1]]\n",
    "\n",
    "# total number of drawers\n",
    "original_drawers = np.arange(1, 21)\n",
    "\n",
    "print(\"There are {} alphabets.\".format(len(original_alphabets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 40 alphabets, randomly select 30\n",
    "train_alphabets = np.random.choice(original_alphabets, size=30, replace=False)\n",
    "remaining_alphabets = [x for x in original_alphabets if x not in train_alphabets]\n",
    "\n",
    "# from 20 drawers, randomly select 12\n",
    "train_drawers = np.random.choice(np.arange(20), size=12, replace=False)\n",
    "remaining_drawers = [x for x in original_drawers if x not in train_drawers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:16<00:00, 889.47it/s]\n"
     ]
    }
   ],
   "source": [
    "num_iters = int(30e3 / 2)\n",
    "cnt = True\n",
    "\n",
    "img_pairs = []\n",
    "label_pairs = []\n",
    "for i in trange(num_iters):\n",
    "    # sample a like pair\n",
    "    if i % 2 == 0:\n",
    "        # uniformly select 1 alphabet\n",
    "        alph = np.random.choice(train_alphabets)\n",
    "                \n",
    "        # uniformly sample 1 character\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        char = np.random.choice(chars)\n",
    "                \n",
    "        # uniformly sample 2 drawers\n",
    "        ds = np.random.choice(train_drawers, size=2, replace=False)\n",
    "                \n",
    "        # get list of filenames to read in char dir\n",
    "        filenames = [\n",
    "            os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "            ) in ds\n",
    "        ]\n",
    "        \n",
    "        # load pair as numpy array and store\n",
    "        pair = []\n",
    "        for name in filenames:\n",
    "            img_arr = img2array(name, gray=True, expand=True)\n",
    "            img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "            pair.append(img_arr)        \n",
    "        img_pairs.append(np.concatenate(pair, axis=0))\n",
    "        \n",
    "        # store ground truth lbl\n",
    "        gd_truth = np.array([1], dtype=np.int64)\n",
    "        label_pairs.append(gd_truth)\n",
    "        \n",
    "    # sample a dissimilar pair\n",
    "    else:\n",
    "        if cnt:\n",
    "            cnt = False\n",
    "\n",
    "            # uniformly select 1 alphabet\n",
    "            alph = np.random.choice(train_alphabets)\n",
    "\n",
    "             # uniformly sample 2 characters\n",
    "            chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "            chars = np.random.choice(chars, size=2, replace=False)\n",
    "\n",
    "            # uniformly sample 1 drawer\n",
    "            d = np.random.choice(train_drawers)\n",
    "\n",
    "            filenames = []\n",
    "            for c in chars:\n",
    "                # get list of filenames to read in char dir\n",
    "                name = [\n",
    "                    os.path.join(c, x) for x in next(os.walk(c))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*name)\n",
    "            \n",
    "            # load pair as numpy array and store\n",
    "            pair = []\n",
    "            for name in filenames:\n",
    "                img_arr = img2array(name, gray=True, expand=True)\n",
    "                img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                pair.append(img_arr)        \n",
    "            img_pairs.append(np.concatenate(pair, axis=0))\n",
    "            \n",
    "             # store ground truth lbl\n",
    "            gd_truth = np.array([0], dtype=np.int64)\n",
    "            label_pairs.append(gd_truth)\n",
    "        else:\n",
    "            cnt = True\n",
    "            \n",
    "            # uniformly select 2 alphabets\n",
    "            alph = np.random.choice(train_alphabets, size=2, replace=False)\n",
    "            \n",
    "            # uniformly sample 1 drawer\n",
    "            d = np.random.choice(train_drawers)\n",
    "            \n",
    "            filenames = []\n",
    "            for a in alph:\n",
    "                # uniformly sample 1 character\n",
    "                chars = [os.path.join(a, x) for x in next(os.walk(a))[1]]\n",
    "                char = np.random.choice(chars)\n",
    "\n",
    "                # get list of filenames to read in char dir\n",
    "                name = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*name)\n",
    "\n",
    "            # load pair as numpy array and store\n",
    "            pair = []\n",
    "            for name in filenames:\n",
    "                img_arr = img2array(name, gray=True, expand=True)\n",
    "                img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                pair.append(img_arr)        \n",
    "            img_pairs.append(np.concatenate(pair, axis=0))\n",
    "            \n",
    "             # store ground truth lbl\n",
    "            gd_truth = np.array([0], dtype=np.int64)\n",
    "            label_pairs.append(gd_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels to prevent monotone (same, different) sequence\n",
    "indices = list(range(len(img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "img_pairs = [img_pairs[idx] for idx in indices]\n",
    "label_pairs = [label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1323735142 total bytes\n",
      "Writing bytes [0, 1073741824]\n",
      "Writing bytes [1073741824, 1323735142]\n",
      "Done!\n",
      "Writing 570223 total bytes\n",
      "Writing bytes [0, 570223]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pickle_dump(img_pairs, data_dir + 'X_train.p')\n",
    "pickle_dump(label_pairs, data_dir + 'y_train.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data Augmentation\n",
    "\n",
    "We add 8 transforms for each training example pair in `img_pair`. The transformation is affine with the following constraints on its parameters:\n",
    "\n",
    "- theta $\\in$ [-10, 10] uniform (rotation)\n",
    "- $\\rho_x$ and $\\rho_y$ $\\in$ [-0.3, 0.3] uniform (shear)\n",
    "- $s_x$ and $s_y$ $\\in$ [0.8, 1.2] uniform (scale)\n",
    "- $t_x$ and $t_y$ $\\in$ [-2, 2] uniform (translation)\n",
    "\n",
    "Each of these parameters is included with probability 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:37<00:00, 402.57it/s]\n"
     ]
    }
   ],
   "source": [
    "arr2pil = transforms.ToPILImage()\n",
    "\n",
    "def pil2array(im):\n",
    "    x = np.asarray(im, dtype=np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255\n",
    "    return x\n",
    "\n",
    "augmented_img_pairs = []\n",
    "augmented_label_pairs = []\n",
    "for idx in trange(len(img_pairs)):\n",
    "    # get gd truth label\n",
    "    label = label_pairs[idx]\n",
    "    \n",
    "    # grab img pair\n",
    "    pair = img_pairs[idx]\n",
    "    pair = np.transpose(pair, (0, 2, 3, 1))\n",
    "    im1, im2 = np.array(pair)\n",
    "    \n",
    "    # convert back to [0, 255] range\n",
    "    im1 *= 255\n",
    "    im2 *= 255\n",
    "    \n",
    "    # transform to PIL image\n",
    "    im1, im2 = arr2pil(im1), arr2pil(im2)\n",
    "    \n",
    "    # compose 8 transforms\n",
    "    for i in range(8):\n",
    "        # randomly select transform with proba 0.5\n",
    "        rot = random.choice([0, [-10, 10]])\n",
    "        shear = random.choice([None, [-0.3, 0.3]])\n",
    "        scale = random.choice([None, [0.8, 1.2]])\n",
    "        trans = random.choice([None, [2/150, 2/150]]) # absolute value\n",
    "        \n",
    "        # apply affine transformation\n",
    "        aff = transforms.RandomAffine(rot, trans, scale, shear)\n",
    "        aug_im1, aug_im2 = aff(im1), aff(im2)\n",
    "        \n",
    "        # convert to numpy array\n",
    "        aug_im1 = pil2array(aug_im1)\n",
    "        aug_im2 = pil2array(aug_im2)\n",
    "        \n",
    "        # transpose to C,H,W\n",
    "        aug_im1 = np.transpose(aug_im1, (0, 3, 1, 2))\n",
    "        aug_im2 = np.transpose(aug_im2, (0, 3, 1, 2))\n",
    "        \n",
    "        # add to list\n",
    "        aug_pairs = np.concatenate([aug_im1, aug_im2], axis=0)\n",
    "        augmented_img_pairs.append(aug_pairs)\n",
    "        augmented_label_pairs.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Train and Augmented Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels\n",
    "indices = list(range(len(augmented_img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "augmented_img_pairs = [augmented_img_pairs[idx] for idx in indices]\n",
    "augmented_label_pairs = [augmented_label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective Train Size: 270000\n"
     ]
    }
   ],
   "source": [
    "train_img_pairs = img_pairs + augmented_img_pairs\n",
    "train_label_pairs = label_pairs + augmented_label_pairs\n",
    "\n",
    "print(\"Effective Train Size: {}\".format(2 * len(train_img_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 11913615382 total bytes\n",
      "Writing bytes [0, 1073741824]\n",
      "Writing bytes [1073741824, 2147483648]\n",
      "Writing bytes [2147483648, 3221225472]\n",
      "Writing bytes [3221225472, 4294967296]\n",
      "Writing bytes [4294967296, 5368709120]\n",
      "Writing bytes [5368709120, 6442450944]\n",
      "Writing bytes [6442450944, 7516192768]\n",
      "Writing bytes [7516192768, 8589934592]\n",
      "Writing bytes [8589934592, 9663676416]\n",
      "Writing bytes [9663676416, 10737418240]\n",
      "Writing bytes [10737418240, 11811160064]\n",
      "Writing bytes [11811160064, 11913615382]\n",
      "Done!\n",
      "Writing 1169584 total bytes\n",
      "Writing bytes [0, 1169584]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pickle_dump(train_img_pairs, data_dir + 'X_train_aug.p')\n",
    "pickle_dump(train_label_pairs, data_dir + 'y_train_aug.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data\n",
    "\n",
    "The validation data consists of 10 alphabets and 4 drawers.\n",
    "\n",
    "The authors used 2 types of validation strategies for early-stopping of the model training. One of the method consists in creating a validation one-shot scenario to test the model's ability to generalize. We pick an alphabet from among the 10 available, choose 16 characters uniformly at random and select 2 of the 4 available drawers. We then select all the 16 characters produced by the first drawer, and individually compare against all 16 characters from the second drawer, with the goal of predicting the class of the character from among all of the second drawer's characters.\n",
    "\n",
    "This process is repeated twice for all alphabets (the second time we pick the 2 other drawers), so that there are 32 one-shot learning trials for each of the 10 validation alphabets, for a total of 320 one-shot trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the 40 alphabets, select the last 10\n",
    "valid_alphabets = remaining_alphabets\n",
    "\n",
    "# from remaining 8 drawers, select 4\n",
    "valid_drawers = np.random.choice(remaining_drawers, size=4, replace=False)\n",
    "remaining_drawers = [x for x in remaining_drawers if x not in valid_drawers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = len(valid_alphabets)\n",
    "\n",
    "# number of characters to sample in each alphabet\n",
    "# I can't increase indefinitely since some alphabets\n",
    "# have less characters than others\n",
    "pop = 13\n",
    "\n",
    "valid_img_pairs = []\n",
    "valid_label_pairs = []\n",
    "for alph in valid_alphabets:\n",
    "    for j in range(2):\n",
    "        # grab drawers\n",
    "        ds = [valid_drawers[2*j], valid_drawers[2*j + 1]]\n",
    "        \n",
    "        # sample 10 characters uniformly\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        chars = np.random.choice(chars, size=pop, replace=False)\n",
    "        \n",
    "        # grab filenames for both drawers\n",
    "        filenames = []\n",
    "        for d in ds:\n",
    "            for char in chars:\n",
    "                names = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*names)\n",
    "        d1 = filenames[:pop]\n",
    "        d2 = filenames[pop:]\n",
    "\n",
    "        for i, left in enumerate(d1):\n",
    "            for right in d2:\n",
    "                img_names = [left, right]\n",
    "                pair = []\n",
    "                for name in img_names:\n",
    "                    img_arr = img2array(name, gray=True, expand=True)\n",
    "                    img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                    pair.append(img_arr) \n",
    "                # create img and label\n",
    "                pair = np.concatenate(pair, axis=0)\n",
    "                label = np.array([i], dtype=np.int64)\n",
    "                \n",
    "                # store\n",
    "                valid_img_pairs.append(pair)\n",
    "                valid_label_pairs.append(label)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels\n",
    "indices = list(range(len(valid_img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "valid_img_pairs = [valid_img_pairs[idx] for idx in indices]\n",
    "valid_label_pairs = [valid_label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 298281740 total bytes\n",
      "Writing bytes [0, 298281740]\n",
      "Done!\n",
      "Writing 128578 total bytes\n",
      "Writing bytes [0, 128578]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pickle_dump(valid_img_pairs, data_dir + 'X_valid.p')\n",
    "pickle_dump(valid_label_pairs, data_dir + 'y_valid.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "The test set consists in 10 alphabets and 4 drawers (just like the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of alphabets\n",
    "test_alphabets = [os.path.join(test_dir, x) for x in next(os.walk(test_dir))[1]]\n",
    "\n",
    "# there are 20 drawers\n",
    "test_drawers = np.arange(1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = len(test_alphabets)\n",
    "\n",
    "# number of characters to sample in each alphabet\n",
    "pop = 20\n",
    "\n",
    "test_img_pairs = []\n",
    "test_label_pairs = []\n",
    "for alph in test_alphabets:\n",
    "    for j in range(2):\n",
    "        # sample a pair of drawers\n",
    "        ds = np.random.choice(test_drawers, size=2, replace=False)\n",
    "        \n",
    "        # sample 20 characters uniformly\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        chars = np.random.choice(chars, size=pop, replace=False)\n",
    "        \n",
    "        # grab filenames for both drawers\n",
    "        filenames = []\n",
    "        for d in ds:\n",
    "            for char in chars:\n",
    "                names = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*names)\n",
    "        d1 = filenames[:pop]\n",
    "        d2 = filenames[pop:]\n",
    "\n",
    "        for i, left in enumerate(d1):\n",
    "            for right in d2:\n",
    "                img_names = [left, right]\n",
    "                pair = []\n",
    "                for name in img_names:\n",
    "                    img_arr = img2array(name, gray=True, expand=True)\n",
    "                    img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                    pair.append(img_arr) \n",
    "                # create img and label\n",
    "                pair = np.concatenate(pair, axis=0)\n",
    "                label = np.array([i], dtype=np.int64)\n",
    "                \n",
    "                # store\n",
    "                test_img_pairs.append(pair)\n",
    "                test_label_pairs.append(label)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels\n",
    "indices = list(range(len(test_img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "test_img_pairs = [test_img_pairs[idx] for idx in indices]\n",
    "test_label_pairs = [test_label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 705992128 total bytes\n",
      "Writing bytes [0, 705992128]\n",
      "Done!\n",
      "Writing 304173 total bytes\n",
      "Writing bytes [0, 304173]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pickle_dump(test_img_pairs, data_dir + 'X_test.p')\n",
    "pickle_dump(test_label_pairs, data_dir + 'y_test.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
