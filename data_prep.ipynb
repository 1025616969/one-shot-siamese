{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/processed/'\n",
    "train_dir = data_dir + 'background/'\n",
    "test_dir = data_dir + 'evaluation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data\n",
    "\n",
    "The train data consists of 40 alphabets and 12 drawers.\n",
    "\n",
    "In each alphabet folder, there is 1 folder for each character in the alphabet. In each character folder, there are 20 images of this character drawn by each of the 20 drawers.\n",
    " \n",
    "- To sample a like pair, first I randomly select an alphabet from the list of training alphabets (uniform proba). Then, I randomly select a character from the list of characters in this alphabet (uniform proba). Then, I randomly select 2 drawers from the list of training drawers (uniform sampling) and select their corresponding image from the subdirectory.\n",
    "- To sample a dissimilar pair, I can do 2 things. The first method is to uniformly sample 1 alphabet, then uniformly sample 2 characters in the alphabet, uniformly sample 1 drawer (or 2) and index 1 drawer per alphabet to obtain 2 different characters from the same alphabet. The second method is to uniformly sample 2 alphabets, uniformly sample a character in each alphabet, uniformly sample a drawer per alphabet (or the same drawer), and obtain 2 different characters from 2 different alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of alphabets\n",
    "original_alphabets = [os.path.join(train_dir, x) for x in next(os.walk(train_dir))[1]]\n",
    "\n",
    "# total number of drawers\n",
    "original_drawers = np.arange(1, 21)\n",
    "\n",
    "print(\"There are {} alphabets.\".format(len(original_alphabets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 40 alphabets, randomly select 30\n",
    "train_alphabets = np.random.choice(original_alphabets, size=30, replace=False)\n",
    "remaining_alphabets = [x for x in original_alphabets if x not in train_alphabets]\n",
    "\n",
    "# from 20 drawers, randomly select 12\n",
    "train_drawers = np.random.choice(np.arange(20), size=12, replace=False)\n",
    "remaining_drawers = [x for x in original_drawers if x not in train_drawers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = int(30e3 / 2)\n",
    "\n",
    "img_pairs = []\n",
    "label_pairs = []\n",
    "for i in trange(10):\n",
    "    # sample a like pair\n",
    "    if i % 2 == 0:\n",
    "        # uniformly select 1 alphabet\n",
    "        alph = np.random.choice(train_alphabets)\n",
    "                \n",
    "        # uniformly sample 1 character\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        char = np.random.choice(chars)\n",
    "                \n",
    "        # uniformly sample 2 drawers\n",
    "        ds = np.random.choice(train_drawers, size=2, replace=True)\n",
    "                \n",
    "        # get list of filenames to read in char dir\n",
    "        filenames = [\n",
    "            os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "            ) in ds\n",
    "        ]\n",
    "        \n",
    "        # in case I get the same drawer\n",
    "        if len(filenames) == 1:\n",
    "            filenames = filenames * 2\n",
    "        \n",
    "        # load pair as numpy array and store\n",
    "        pair = []\n",
    "        for name in filenames:\n",
    "            img_arr = img2array(name, gray=True, expand=True)\n",
    "            img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "            pair.append(img_arr)        \n",
    "        img_pairs.append(np.concatenate(pair, axis=0))\n",
    "        \n",
    "        # store ground truth lbl\n",
    "        gd_truth = np.array([1], dtype=np.int64)\n",
    "        label_pairs.append(gd_truth)\n",
    "        \n",
    "    # sample a dissimilar pair\n",
    "    else:\n",
    "        redo = True\n",
    "        while redo:\n",
    "            # uniformly select 2 alphabets\n",
    "            alph = np.random.choice(train_alphabets, size=2, replace=True)\n",
    "\n",
    "            # uniformly sample 1 drawer\n",
    "            ds = np.random.choice(train_drawers, size=2, replace=True)\n",
    "\n",
    "            filenames = []\n",
    "            for i, a in enumerate(alph):\n",
    "                # uniformly sample 1 character\n",
    "                chars = [os.path.join(a, x) for x in next(os.walk(a))[1]]\n",
    "                char = np.random.choice(chars)\n",
    "\n",
    "                # get list of filenames to read in char dir\n",
    "                name = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == ds[i]\n",
    "                ]\n",
    "                filenames.append(*name)\n",
    "            \n",
    "            # in case I sample the same image\n",
    "            redo = True if len(list(set(filenames))) == 1 else False\n",
    "            if redo:\n",
    "                print(\"redoing!\")\n",
    "\n",
    "        # load pair as numpy array and store\n",
    "        pair = []\n",
    "        for name in filenames:\n",
    "            img_arr = img2array(name, gray=True, expand=True)\n",
    "            img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "            pair.append(img_arr)        \n",
    "        img_pairs.append(np.concatenate(pair, axis=0))\n",
    "\n",
    "         # store ground truth lbl\n",
    "        gd_truth = np.array([0], dtype=np.int64)\n",
    "        label_pairs.append(gd_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels to prevent monotone (same, different) sequence\n",
    "indices = list(range(len(img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "img_pairs = [img_pairs[idx] for idx in indices]\n",
    "label_pairs = [label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(img_pairs, data_dir + 'X_train.p')\n",
    "pickle_dump(label_pairs, data_dir + 'y_train.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data Augmentation\n",
    "\n",
    "We add 8 transforms for each training example pair in `img_pair`. The transformation is affine with the following constraints on its parameters:\n",
    "\n",
    "- theta $\\in$ [-10, 10] uniform (rotation)\n",
    "- $\\rho_x$ and $\\rho_y$ $\\in$ [-0.3, 0.3] uniform (shear)\n",
    "- $s_x$ and $s_y$ $\\in$ [0.8, 1.2] uniform (scale)\n",
    "- $t_x$ and $t_y$ $\\in$ [-2, 2] uniform (translation)\n",
    "\n",
    "Each of these parameters is included with probability 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2pil = transforms.ToPILImage()\n",
    "\n",
    "def pil2array(im):\n",
    "    x = np.asarray(im, dtype=np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255\n",
    "    return x\n",
    "\n",
    "augmented_img_pairs = []\n",
    "augmented_label_pairs = []\n",
    "for idx in trange(len(img_pairs)):\n",
    "    # get gd truth label\n",
    "    label = label_pairs[idx]\n",
    "    \n",
    "    # grab img pair\n",
    "    pair = img_pairs[idx]\n",
    "    pair = np.transpose(pair, (0, 2, 3, 1))\n",
    "    im1, im2 = np.array(pair)\n",
    "    \n",
    "    # convert back to [0, 255] range\n",
    "    im1 *= 255\n",
    "    im2 *= 255\n",
    "    \n",
    "    # transform to PIL image\n",
    "    im1, im2 = arr2pil(im1), arr2pil(im2)\n",
    "    \n",
    "    # compose 8 transforms\n",
    "    for i in range(8):\n",
    "        # randomly select transform with proba 0.5\n",
    "        rot = random.choice([0, [-10, 10]])\n",
    "        shear = random.choice([None, [-0.3, 0.3]])\n",
    "        scale = random.choice([None, [0.8, 1.2]])\n",
    "        trans = random.choice([None, [2/150, 2/150]]) # absolute value\n",
    "        \n",
    "        # apply affine transformation\n",
    "        aff = transforms.RandomAffine(rot, trans, scale, shear)\n",
    "        aug_im1, aug_im2 = aff(im1), aff(im2)\n",
    "        \n",
    "        # convert to numpy array\n",
    "        aug_im1 = pil2array(aug_im1)\n",
    "        aug_im2 = pil2array(aug_im2)\n",
    "        \n",
    "        # transpose to C,H,W\n",
    "        aug_im1 = np.transpose(aug_im1, (0, 3, 1, 2))\n",
    "        aug_im2 = np.transpose(aug_im2, (0, 3, 1, 2))\n",
    "        \n",
    "        # add to list\n",
    "        aug_pairs = np.concatenate([aug_im1, aug_im2], axis=0)\n",
    "        augmented_img_pairs.append(aug_pairs)\n",
    "        augmented_label_pairs.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Train and Augmented Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels\n",
    "indices = list(range(len(augmented_img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "augmented_img_pairs = [augmented_img_pairs[idx] for idx in indices]\n",
    "augmented_label_pairs = [augmented_label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_pairs = img_pairs + augmented_img_pairs\n",
    "train_label_pairs = label_pairs + augmented_label_pairs\n",
    "\n",
    "print(\"Effective Train Size: {}\".format(2 * len(train_img_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(train_img_pairs, data_dir + 'X_train_aug.p')\n",
    "pickle_dump(train_label_pairs, data_dir + 'y_train_aug.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data\n",
    "\n",
    "The validation data consists of 10 alphabets and 4 drawers.\n",
    "\n",
    "The authors used 2 types of validation strategies for early-stopping of the model training. One of the method consists in creating a validation one-shot scenario to test the model's ability to generalize. We pick an alphabet from among the 10 available, choose 16 characters uniformly at random and select 2 of the 4 available drawers. We then select all the 16 characters produced by the first drawer, and individually compare against all 16 characters from the second drawer, with the goal of predicting the class of the character from among all of the second drawer's characters.\n",
    "\n",
    "This process is repeated twice for all alphabets (the second time we pick the 2 other drawers), so that there are 32 one-shot learning trials for each of the 10 validation alphabets, for a total of 320 one-shot trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the 40 alphabets, select the last 10\n",
    "valid_alphabets = remaining_alphabets\n",
    "\n",
    "# from remaining 8 drawers, select 4\n",
    "valid_drawers = np.random.choice(remaining_drawers, size=4, replace=False)\n",
    "remaining_drawers = [x for x in remaining_drawers if x not in valid_drawers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = len(valid_alphabets)\n",
    "\n",
    "# number of characters to sample in each alphabet\n",
    "pop = 14\n",
    "\n",
    "valid_img_pairs = []\n",
    "valid_label_pairs = []\n",
    "for alph in valid_alphabets[0:1]:\n",
    "    for j in range(2):\n",
    "        # grab drawers\n",
    "        ds = [valid_drawers[2*j], valid_drawers[2*j + 1]]\n",
    "        \n",
    "        # sample pop characters uniformly\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        chars = np.random.choice(chars, size=pop, replace=False)\n",
    "        \n",
    "        # grab filenames for both drawers\n",
    "        filenames = []\n",
    "        for d in ds:\n",
    "            for char in chars:\n",
    "                names = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*names)\n",
    "        d1 = filenames[:pop]\n",
    "        d2 = filenames[pop:]\n",
    "        \n",
    "        \n",
    "        for i, left in enumerate(d1):\n",
    "            way_pairs = []\n",
    "            way_labels = []\n",
    "            for right in d2:\n",
    "                img_names = [left, right]\n",
    "                pair = []\n",
    "                for name in img_names:\n",
    "                    img_arr = img2array(name, gray=True, expand=True)\n",
    "                    img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                    pair.append(img_arr) \n",
    "                # create img and store\n",
    "                pair = np.concatenate(pair, axis=0)\n",
    "                way_pairs.append(pair)\n",
    "            \n",
    "            # create pop-way task\n",
    "            way_pairs = [np.expand_dims(x, axis=0) for x in way_pairs]\n",
    "            way_pairs = np.concatenate(way_pairs, axis=0)\n",
    "            valid_img_pairs.append(way_pairs)\n",
    "            label = np.array([i], dtype=np.int64)\n",
    "            valid_label_pairs.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels\n",
    "indices = list(range(len(valid_img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "valid_img_pairs = [valid_img_pairs[idx] for idx in indices]\n",
    "valid_label_pairs = [valid_label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(valid_img_pairs, data_dir + 'X_valid.p')\n",
    "pickle_dump(valid_label_pairs, data_dir + 'y_valid.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "The test set consists in 10 alphabets and 4 drawers (just like the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of alphabets\n",
    "test_alphabets = [os.path.join(test_dir, x) for x in next(os.walk(test_dir))[1]]\n",
    "\n",
    "# there are 20 drawers\n",
    "test_drawers = np.arange(1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = len(test_alphabets)\n",
    "\n",
    "# number of characters to sample in each alphabet\n",
    "pop = 20\n",
    "\n",
    "test_img_pairs = []\n",
    "test_label_pairs = []\n",
    "for alph in test_alphabets:\n",
    "    for j in range(2):\n",
    "        # sample a pair of drawers\n",
    "        ds = np.random.choice(test_drawers, size=2, replace=False)\n",
    "        \n",
    "        # sample 20 characters uniformly\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        chars = np.random.choice(chars, size=pop, replace=False)\n",
    "        \n",
    "        # grab filenames for both drawers\n",
    "        filenames = []\n",
    "        for d in ds:\n",
    "            for char in chars:\n",
    "                names = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*names)\n",
    "        d1 = filenames[:pop]\n",
    "        d2 = filenames[pop:]\n",
    "\n",
    "        for i, left in enumerate(d1):\n",
    "            way_pairs = []\n",
    "            way_labels = []\n",
    "            for right in d2:\n",
    "                img_names = [left, right]\n",
    "                pair = []\n",
    "                for name in img_names:\n",
    "                    img_arr = img2array(name, gray=True, expand=True)\n",
    "                    img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                    pair.append(img_arr) \n",
    "               # create img and store\n",
    "                pair = np.concatenate(pair, axis=0)\n",
    "                way_pairs.append(pair)\n",
    "                \n",
    "            # create pop-way task\n",
    "            way_pairs = [np.expand_dims(x, axis=0) for x in way_pairs]\n",
    "            way_pairs = np.concatenate(way_pairs, axis=0)\n",
    "            test_img_pairs.append(way_pairs)\n",
    "            label = np.array([i], dtype=np.int64)\n",
    "            test_label_pairs.append(label)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels\n",
    "indices = list(range(len(test_img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "test_img_pairs = [test_img_pairs[idx] for idx in indices]\n",
    "test_label_pairs = [test_label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(test_img_pairs, data_dir + 'X_test.p')\n",
    "pickle_dump(test_label_pairs, data_dir + 'y_test.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train One-Shot Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = len(train_alphabets)\n",
    "\n",
    "# number of characters to sample in each alphabet\n",
    "pop = 12\n",
    "\n",
    "train_img_pairs = []\n",
    "train_label_pairs = []\n",
    "for alph in train_alphabets:\n",
    "    for j in range(2):\n",
    "        # grab drawers\n",
    "        ds = [train_drawers[2*j], train_drawers[2*j + 1]]\n",
    "        \n",
    "        # sample pop characters uniformly\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        chars = np.random.choice(chars, size=pop, replace=False)\n",
    "        \n",
    "        # grab filenames for both drawers\n",
    "        filenames = []\n",
    "        for d in ds:\n",
    "            for char in chars:\n",
    "                names = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*names)\n",
    "        d1 = filenames[:pop]\n",
    "        d2 = filenames[pop:]\n",
    "        \n",
    "        \n",
    "        for i, left in enumerate(d1):\n",
    "            way_pairs = []\n",
    "            way_labels = []\n",
    "            for right in d2:\n",
    "                img_names = [left, right]\n",
    "                pair = []\n",
    "                for name in img_names:\n",
    "                    img_arr = img2array(name, gray=True, expand=True)\n",
    "                    img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                    pair.append(img_arr) \n",
    "                # create img and store\n",
    "                pair = np.concatenate(pair, axis=0)\n",
    "                way_pairs.append(pair)\n",
    "            \n",
    "            # create pop-way task\n",
    "            way_pairs = [np.expand_dims(x, axis=0) for x in way_pairs]\n",
    "            way_pairs = np.concatenate(way_pairs, axis=0)\n",
    "            train_img_pairs.append(way_pairs)\n",
    "            label = np.array([i], dtype=np.int64)\n",
    "            train_label_pairs.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lake et al. Test Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = data_dir + 'all_runs/'\n",
    "fname_label = 'class_labels.txt'\n",
    "num_trials = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pairs = []\n",
    "run_labels = []\n",
    "\n",
    "for i in range(1, num_trials+1):\n",
    "    rs = str(i)\n",
    "    if len(rs) == 1:\n",
    "        rs = '0' + rs\n",
    "    rs += '/'\n",
    "    folder_name = folder_dir + 'run' + rs\n",
    "\n",
    "    # get file names\n",
    "    with open(folder_name + fname_label) as f:\n",
    "        content = f.read().splitlines()\n",
    "    pairs = [line.split() for line in content]\n",
    "    test_files  = [pair[0] for pair in pairs]\n",
    "    train_files = [pair[1] for pair in pairs]\n",
    "    test_files = [os.path.join(folder_dir, x) for x in test_files]\n",
    "    train_files = [os.path.join(folder_dir, x) for x in train_files]\n",
    "    answers_files = copy.copy(train_files)\n",
    "    test_files.sort()\n",
    "    train_files.sort()\n",
    "\n",
    "    for j, test_name in enumerate(test_files):\n",
    "        img_pairs = []\n",
    "        for train_name in train_files:\n",
    "            filenames = [test_name, train_name]\n",
    "            pair = []\n",
    "            for name in filenames:\n",
    "                img_arr = img2array(name, gray=True, expand=True)\n",
    "                img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                pair.append(img_arr)\n",
    "            pair = np.concatenate(pair, axis=0)\n",
    "            img_pairs.append(pair)\n",
    "        img_pairs = [np.expand_dims(x, axis=0) for x in img_pairs]\n",
    "        img_pairs = np.concatenate(img_pairs, axis=0)\n",
    "        run_pairs.append(img_pairs)\n",
    "        label = np.array([int(answers_files[j][-6:-4])-1], dtype=np.int64)\n",
    "        run_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 705620514 total bytes\n",
      "Writing bytes [0, 705620514]\n",
      "Done!\n",
      "Writing 15323 total bytes\n",
      "Writing bytes [0, 15323]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pickle_dump(run_pairs, data_dir + 'X_test.p')\n",
    "pickle_dump(run_labels, data_dir + 'y_test.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
